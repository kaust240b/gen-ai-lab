{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YniLyPZcGkX"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (optional - for saving models)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "vdkloBTBchFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Dataset Preparation"
      ],
      "metadata": {
        "id": "T7sJW2kCfTEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The provided text corpus\n",
        "text_corpus = \"\"\"artificial intelligence is transforming modern society.\n",
        "it is used in healthcare finance education and transportation.\n",
        "machine learning allows systems to improve automatically with experience.\n",
        "data plays a critical role in training intelligent systems.\n",
        "large datasets help models learn complex patterns.\n",
        "deep learning uses multi layer neural networks.\n",
        "neural networks are inspired by biological neurons.\n",
        "each neuron processes input and produces an output.\n",
        "training a neural network requires optimization techniques.\n",
        "gradient descent minimizes the loss function.\n",
        "\n",
        "natural language processing helps computers understand human language.\n",
        "text generation is a key task in nlp.\n",
        "language models predict the next word or character.\n",
        "recurrent neural networks handle sequential data.\n",
        "lstm and gru models address long term dependency problems.\n",
        "however rnn based models are slow for long sequences.\n",
        "\n",
        "transformer models changed the field of nlp.\n",
        "they rely on self attention mechanisms.\n",
        "attention allows the model to focus on relevant context.\n",
        "transformers process data in parallel.\n",
        "this makes training faster and more efficient.\n",
        "modern language models are based on transformers.\n",
        "\n",
        "education is being improved using artificial intelligence.\n",
        "intelligent tutoring systems personalize learning.\n",
        "automated grading saves time for teachers.\n",
        "online education platforms use recommendation systems.\n",
        "technology enhances the quality of learning experiences.\n",
        "\n",
        "ethical considerations are important in artificial intelligence.\n",
        "fairness transparency and accountability must be ensured.\n",
        "ai systems should be designed responsibly.\n",
        "data privacy and security are major concerns.\n",
        "researchers continue to improve ai safety.\n",
        "\n",
        "text generation models can create stories poems and articles.\n",
        "they are used in chatbots virtual assistants and content creation.\n",
        "generated text should be meaningful and coherent.\n",
        "evaluation of text generation is challenging.\n",
        "human judgement is often required.\n",
        "\n",
        "continuous learning is essential in the field of ai.\n",
        "research and innovation drive technological progress.\n",
        "students should build strong foundations in mathematics.\n",
        "programming skills are important for ai engineers.\n",
        "practical experimentation enhances understanding.\"\"\"\n",
        "\n",
        "print(f\"Total characters in corpus: {len(text_corpus)}\")\n",
        "print(f\"First 200 characters:\\n{text_corpus[:200]}\")"
      ],
      "metadata": {
        "id": "nGw8osw2fI5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Character-Level Tokenization and Preprocessing"
      ],
      "metadata": {
        "id": "fLq_am5DfUFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharTokenizer:\n",
        "    \"\"\"\n",
        "    Character-level tokenizer for text generation.\n",
        "    Creates mappings between characters and numerical indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, text):\n",
        "        # Get unique characters and create sorted vocabulary\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "        # Create mappings: char -> index and index -> char\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"Characters: {''.join(self.chars)}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to list of indices\"\"\"\n",
        "        return [self.char_to_idx[ch] for ch in text]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert list of indices back to text\"\"\"\n",
        "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = CharTokenizer(text_corpus)\n",
        "\n",
        "# Encode entire text\n",
        "encoded_text = tokenizer.encode(text_corpus)\n",
        "print(f\"\\nEncoded text length: {len(encoded_text)}\")\n",
        "print(f\"First 20 encoded values: {encoded_text[:20]}\")"
      ],
      "metadata": {
        "id": "JqkywsSWfI29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Create Training Sequences"
      ],
      "metadata": {
        "id": "mFdTRKVRfiwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(encoded_text, seq_length):\n",
        "    \"\"\"\n",
        "    Create input-output sequences for training.\n",
        "    Input: sequence of characters\n",
        "    Output: next character after the sequence\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Slide window across text\n",
        "    for i in range(len(encoded_text) - seq_length):\n",
        "        # Input sequence\n",
        "        input_seq = encoded_text[i:i + seq_length]\n",
        "        # Target is the next character\n",
        "        target_char = encoded_text[i + seq_length]\n",
        "\n",
        "        X.append(input_seq)\n",
        "        y.append(target_char)\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Hyperparameters for sequence creation\n",
        "SEQUENCE_LENGTH = 50  # Length of input sequences\n",
        "\n",
        "# Create sequences\n",
        "X, y = create_sequences(encoded_text, SEQUENCE_LENGTH)\n",
        "\n",
        "print(f\"Total sequences: {len(X)}\")\n",
        "print(f\"Input shape: {X.shape}\")  # (num_sequences, seq_length)\n",
        "print(f\"Target shape: {y.shape}\")  # (num_sequences,)\n",
        "\n",
        "# Show example\n",
        "print(f\"\\nExample sequence:\")\n",
        "print(f\"Input: '{tokenizer.decode(X[0].tolist())}'\")\n",
        "print(f\"Target: '{tokenizer.idx_to_char[y[0].item()]}'\")"
      ],
      "metadata": {
        "id": "bXXukL4NfI0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Create Training Sequences"
      ],
      "metadata": {
        "id": "TNFPnCtrflhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level LSTM for text generation.\n",
        "    Architecture: Embedding -> LSTM -> Linear -> Softmax\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.2):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layer: convert character indices to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # LSTM layers for sequence processing\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,  # Input format: (batch, seq, features)\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer: maps LSTM output to vocabulary space\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        x: input tensor of shape (batch_size, seq_length)\n",
        "        hidden: previous hidden state (for generation)\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Convert indices to embeddings: (batch, seq) -> (batch, seq, embed_dim)\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        # output: (batch, seq, hidden_dim)\n",
        "        # hidden: (num_layers, batch, hidden_dim), (num_layers, batch, hidden_dim)\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        # Take only the last time step output for prediction\n",
        "        # Shape: (batch, hidden_dim)\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "\n",
        "        # Apply dropout\n",
        "        last_output = self.dropout(last_output)\n",
        "\n",
        "        # Map to vocabulary size: (batch, vocab_size)\n",
        "        output = self.fc(last_output)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialize hidden state for generation\"\"\"\n",
        "        # LSTM has two hidden states: hidden state (h) and cell state (c)\n",
        "        h = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return (h, c)\n",
        "\n",
        "# Initialize model\n",
        "EMBED_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "\n",
        "model_lstm = CharLSTM(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
        "\n",
        "print(model_lstm)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_lstm.parameters()):,}\")"
      ],
      "metadata": {
        "id": "wUSa3NcVfIyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Training the LSTM Model"
      ],
      "metadata": {
        "id": "lnsET7qpfxuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X, y, epochs=50, batch_size=64, print_every=10):\n",
        "    \"\"\"\n",
        "    Train the LSTM model on character sequences.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    # Create data loader\n",
        "    dataset = torch.utils.data.TensorDataset(X, y)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Move to device\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output, _ = model(batch_X)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output, batch_y)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"Epoch [{epoch}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "# Train the model\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "losses = train_model(model_lstm, X, y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uZeywHHHfIvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7: Text Generation with LSTM"
      ],
      "metadata": {
        "id": "Ft-2w-FZf5Vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, predict_len=200, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generate text using the trained LSTM model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: trained LSTM model\n",
        "    - tokenizer: character tokenizer\n",
        "    - seed_text: initial text to start generation\n",
        "    - predict_len: number of characters to generate\n",
        "    - temperature: controls randomness (0.1=conservative, 1.0=creative, >1.0=random)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare seed text\n",
        "    if len(seed_text) < SEQUENCE_LENGTH:\n",
        "        # Pad with spaces if seed is too short\n",
        "        seed_text = ' ' * (SEQUENCE_LENGTH - len(seed_text)) + seed_text\n",
        "    else:\n",
        "        seed_text = seed_text[-SEQUENCE_LENGTH:]\n",
        "\n",
        "    generated = seed_text\n",
        "    input_seq = torch.tensor(tokenizer.encode(seed_text), dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # Initialize hidden state\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    # Warm up the model with seed text\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(seed_text) - 1):\n",
        "            _, hidden = model(input_seq[:, i:i+1], hidden)\n",
        "\n",
        "    # Generate new characters\n",
        "    current_input = input_seq[:, -1:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(predict_len):\n",
        "            # Get prediction\n",
        "            output, hidden = model(current_input, hidden)\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            output_dist = output.data.view(-1).div(temperature).exp()\n",
        "\n",
        "            # Sample from the distribution\n",
        "            top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "            # Add to generated text\n",
        "            predicted_char = tokenizer.idx_to_char[top_i.item()]\n",
        "            generated += predicted_char\n",
        "\n",
        "            # Prepare next input\n",
        "            current_input = torch.tensor([[top_i]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Generate text with different seeds and temperatures\n",
        "print(\"=\" * 60)\n",
        "print(\"LSTM TEXT GENERATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "seeds = [\n",
        "    \"artificial intelligence\",\n",
        "    \"machine learning\",\n",
        "    \"deep learning\",\n",
        "    \"natural language\"\n",
        "]\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\n--- Seed: '{seed}' ---\")\n",
        "    generated = generate_text(model_lstm, tokenizer, seed, predict_len=150, temperature=0.8)\n",
        "    print(generated)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Try different temperatures\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EFFECT OF TEMPERATURE ON GENERATION\")\n",
        "print(\"=\" * 60)\n",
        "seed = \"neural networks\"\n",
        "for temp in [0.5, 0.8, 1.0, 1.2]:\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    generated = generate_text(model_lstm, tokenizer, seed, predict_len=100, temperature=temp)\n",
        "    print(generated[:150] + \"...\")"
      ],
      "metadata": {
        "id": "hfT82Pb1fIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save({\n",
        "    'model_state_dict': model_lstm.state_dict(),\n",
        "    'char_to_idx': tokenizer.char_to_idx,\n",
        "    'idx_to_char': tokenizer.idx_to_char,\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'embed_dim': EMBED_DIM,\n",
        "    'hidden_dim': HIDDEN_DIM,\n",
        "    'num_layers': NUM_LAYERS\n",
        "}, '/content/lstm_text_generation_model.pth')\n",
        "\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# To load the model later:\n",
        "\"\"\"\n",
        "checkpoint = torch.load('/content/lstm_text_generation_model.pth')\n",
        "tokenizer_loaded = CharTokenizer(\"\")  # Empty init\n",
        "tokenizer_loaded.char_to_idx = checkpoint['char_to_idx']\n",
        "tokenizer_loaded.idx_to_char = checkpoint['idx_to_char']\n",
        "tokenizer_loaded.vocab_size = checkpoint['vocab_size']\n",
        "tokenizer_loaded.chars = list(checkpoint['char_to_idx'].keys())\n",
        "\n",
        "model_loaded = CharLSTM(\n",
        "    vocab_size=checkpoint['vocab_size'],\n",
        "    embed_dim=checkpoint['embed_dim'],\n",
        "    hidden_dim=checkpoint['hidden_dim'],\n",
        "    num_layers=checkpoint['num_layers']\n",
        ").to(device)\n",
        "model_loaded.load_state_dict(checkpoint['model_state_dict'])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "G5k2AQZfgI8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Component II: Transformer Based Text Generation\n",
        "Cell 9: Transformer Setup"
      ],
      "metadata": {
        "id": "UCMNv5OagPq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Transformer, we'll use a simplified version due to memory constraints\n",
        "# We'll use word-level tokenization for better efficiency\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "class WordTokenizer:\n",
        "    \"\"\"\n",
        "    Word-level tokenizer for Transformer model.\n",
        "    Handles unknown words with <UNK> token.\n",
        "    \"\"\"\n",
        "    def __init__(self, text, max_vocab_size=500):\n",
        "        # Clean and split text into words\n",
        "        words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())\n",
        "\n",
        "        # Count word frequencies\n",
        "        word_counts = Counter(words)\n",
        "\n",
        "        # Keep most common words\n",
        "        most_common = word_counts.most_common(max_vocab_size - 3)  # Reserve spots for special tokens\n",
        "\n",
        "        # Build vocabulary with special tokens\n",
        "        self.vocab = ['<PAD>', '<UNK>', '<EOS>'] + [word for word, _ in most_common]\n",
        "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
        "        self.idx_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"Most common words: {self.vocab[3:13]}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to list of word indices\"\"\"\n",
        "        words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())\n",
        "        return [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in words]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert indices back to text\"\"\"\n",
        "        words = [self.idx_to_word.get(idx, '<UNK>') for idx in indices]\n",
        "        return ' '.join(words)\n",
        "\n",
        "# Create word-level tokenizer\n",
        "word_tokenizer = WordTokenizer(text_corpus, max_vocab_size=300)\n",
        "\n",
        "# Encode text\n",
        "encoded_words = word_tokenizer.encode(text_corpus)\n",
        "print(f\"\\nTotal words: {len(encoded_words)}\")\n",
        "print(f\"First 20 words: {encoded_words[:20]}\")\n",
        "print(f\"Decoded: {word_tokenizer.decode(encoded_words[:20])}\")"
      ],
      "metadata": {
        "id": "ji8kQyWDgPMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10: Transformer Components"
      ],
      "metadata": {
        "id": "vLfMl9YGgWqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional Encoding for Transformer.\n",
        "    Adds position information to embeddings since Transformer has no recurrence.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Calculate angles\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() *\n",
        "            (-np.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        # Apply sin to even indices, cos to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register as buffer (not a parameter, but saved with model)\n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Add positional encoding to input embeddings.\n",
        "        x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer Encoder Block.\n",
        "    Consists of Multi-Head Attention and Feed-Forward Network.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-Attention with residual connection\n",
        "        attn_output, _ = self.attention(x, x, x, attn_mask=mask, need_weights=False)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed-Forward with residual connection\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based Language Model for text generation.\n",
        "    Simplified version suitable for educational purposes.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2,\n",
        "                 d_ff=256, max_seq_len=50, dropout=0.1):\n",
        "        super(TransformerLanguageModel, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Token embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights for stable training\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through Transformer.\n",
        "        x: (batch_size, seq_len) - token indices\n",
        "        \"\"\"\n",
        "        # Create causal mask (prevent looking at future tokens)\n",
        "        seq_len = x.size(1)\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
        "\n",
        "        # Embedding and positional encoding\n",
        "        x = self.embedding(x) * np.sqrt(self.d_model)  # Scale embeddings\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Output projection (only use last position for prediction)\n",
        "        output = self.fc(x[:, -1, :])  # (batch_size, vocab_size)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate(self, tokenizer, seed_text, max_length=30, temperature=0.8):\n",
        "        \"\"\"\n",
        "        Generate text using the trained Transformer.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Encode seed\n",
        "        input_ids = tokenizer.encode(seed_text)\n",
        "\n",
        "        # Truncate if too long\n",
        "        if len(input_ids) > self.max_seq_len:\n",
        "            input_ids = input_ids[-self.max_seq_len:]\n",
        "\n",
        "        generated_words = tokenizer.encode(seed_text)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                # Prepare input\n",
        "                input_tensor = torch.tensor([input_ids[-self.max_seq_len:]], dtype=torch.long).to(device)\n",
        "\n",
        "                # Get prediction\n",
        "                output = self(input_tensor)\n",
        "\n",
        "                # Apply temperature and sample\n",
        "                probs = torch.softmax(output / temperature, dim=-1)\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "                # Stop if end token\n",
        "                if next_token == tokenizer.word_to_idx.get('<EOS>', next_token):\n",
        "                    break\n",
        "\n",
        "                generated_words.append(next_token)\n",
        "                input_ids.append(next_token)\n",
        "\n",
        "        return tokenizer.decode(generated_words)\n",
        "\n",
        "# Initialize Transformer model\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 2\n",
        "D_FF = 256\n",
        "MAX_SEQ_LEN = 30  # Shorter for memory efficiency\n",
        "\n",
        "model_transformer = TransformerLanguageModel(\n",
        "    vocab_size=word_tokenizer.vocab_size,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    d_ff=D_FF,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "print(model_transformer)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_transformer.parameters()):,}\")"
      ],
      "metadata": {
        "id": "6iTES2iafD1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 11: Prepare Transformer Data"
      ],
      "metadata": {
        "id": "nIga4OO4gcxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_sequences(encoded_words, seq_length):\n",
        "    \"\"\"\n",
        "    Create sequences for word-level training.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(encoded_words) - seq_length):\n",
        "        X.append(encoded_words[i:i + seq_length])\n",
        "        y.append(encoded_words[i + seq_length])\n",
        "    return torch.tensor(X, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Create word sequences\n",
        "WORD_SEQ_LEN = 20  # Shorter sequences for Transformer\n",
        "X_words, y_words = create_word_sequences(encoded_words, WORD_SEQ_LEN)\n",
        "\n",
        "print(f\"Total sequences: {len(X_words)}\")\n",
        "print(f\"Input shape: {X_words.shape}\")\n",
        "print(f\"Example input: {word_tokenizer.decode(X_words[0].tolist())}\")\n",
        "print(f\"Example target: '{word_tokenizer.idx_to_word[y_words[0].item()]}'\")"
      ],
      "metadata": {
        "id": "kSDXquargbIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 12: Train Transformer Model"
      ],
      "metadata": {
        "id": "UlJCIFHqgfU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(model, X, y, epochs=50, batch_size=32, lr=0.001):\n",
        "    \"\"\"\n",
        "    Train the Transformer model.\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X, y)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    losses = []\n",
        "    model.train()\n",
        "\n",
        "    print(\"Training Transformer...\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(batch_X)\n",
        "            loss = criterion(output, batch_y)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        losses.append(avg_loss)\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch [{epoch}/{epochs}], Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "# Train\n",
        "TRANSFORMER_EPOCHS = 100\n",
        "transformer_losses = train_transformer(model_transformer, X_words, y_words,\n",
        "                                      epochs=TRANSFORMER_EPOCHS, batch_size=16)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(transformer_losses, label='Transformer')\n",
        "plt.title('Transformer Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zjvnB3iAgT5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 13: Generate Text with Transformer"
      ],
      "metadata": {
        "id": "7zdc5Ah8goga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TRANSFORMER TEXT GENERATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_seeds = [\n",
        "    \"artificial intelligence is\",\n",
        "    \"machine learning allows\",\n",
        "    \"deep learning uses\",\n",
        "    \"neural networks are\"\n",
        "]\n",
        "\n",
        "for seed in test_seeds:\n",
        "    print(f\"\\n--- Seed: '{seed}' ---\")\n",
        "    generated = model_transformer.generate(\n",
        "        word_tokenizer,\n",
        "        seed,\n",
        "        max_length=25,\n",
        "        temperature=0.8\n",
        "    )\n",
        "    print(generated)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Compare temperatures\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TEMPERATURE COMPARISON (Transformer)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "seed = \"natural language processing\"\n",
        "for temp in [0.5, 0.8, 1.0]:\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    generated = model_transformer.generate(word_tokenizer, seed, max_length=20, temperature=temp)\n",
        "    print(generated)"
      ],
      "metadata": {
        "id": "7eNTiv37gg47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 14: Comparison and Analysis"
      ],
      "metadata": {
        "id": "XZnGraWPg304"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, X, y, model_type='lstm'):\n",
        "    \"\"\"\n",
        "    Calculate perplexity as a measure of model quality.\n",
        "    Lower perplexity = better model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X), 32):\n",
        "            batch_X = X[i:i+32].to(device)\n",
        "            batch_y = y[i:i+32].to(device)\n",
        "\n",
        "            if model_type == 'lstm':\n",
        "                output, _ = model(batch_X)\n",
        "            else:\n",
        "                output = model(batch_X)\n",
        "\n",
        "            loss = criterion(output, batch_y)\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += len(batch_y)\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = np.exp(avg_loss)\n",
        "    return perplexity\n",
        "\n",
        "# Calculate perplexities\n",
        "print(\"Calculating model perplexities...\")\n",
        "print(\"Lower perplexity indicates better prediction performance.\\n\")\n",
        "\n",
        "try:\n",
        "    lstm_ppl = calculate_perplexity(model_lstm, X, y, 'lstm')\n",
        "    print(f\"LSTM Perplexity: {lstm_ppl:.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"LSTM Perplexity calculation error: {e}\")\n",
        "\n",
        "try:\n",
        "    transformer_ppl = calculate_perplexity(model_transformer, X_words, y_words, 'transformer')\n",
        "    print(f\"Transformer Perplexity: {transformer_ppl:.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Transformer Perplexity calculation error: {e}\")\n",
        "\n",
        "# Manual quality assessment\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"QUALITATIVE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\"\"\n",
        "LSTM Model Characteristics:\n",
        "- Sequential processing (slower but memory efficient)\n",
        "- Good at capturing local patterns\n",
        "- Struggles with very long dependencies\n",
        "- Character-level: Can learn spelling and basic grammar\n",
        "\n",
        "Transformer Model Characteristics:\n",
        "- Parallel processing (faster training)\n",
        "- Better at long-range dependencies via attention\n",
        "- Requires more memory\n",
        "- Word-level: Better semantic coherence but limited vocabulary\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "no0mIi1jgyWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 15: Interactive Demo"
      ],
      "metadata": {
        "id": "kQZ-LSBYhEJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_generation():\n",
        "    \"\"\"\n",
        "    Interactive text generation demo.\n",
        "    Run this cell and input your own seed text!\n",
        "    \"\"\"\n",
        "    print(\"Interactive Text Generation Demo\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"1. LSTM Model (Character-level)\")\n",
        "    print(\"2. Transformer Model (Word-level)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    choice = input(\"Select model (1 or 2): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        seed = input(\"Enter seed text (at least 20 chars recommended): \").strip()\n",
        "        length = int(input(\"Generation length (characters): \"))\n",
        "        temp = float(input(\"Temperature (0.1-1.5): \"))\n",
        "\n",
        "        print(\"\\nGenerating...\")\n",
        "        result = generate_text(model_lstm, tokenizer, seed, length, temp)\n",
        "        print(\"\\nGenerated Text:\")\n",
        "        print(result)\n",
        "\n",
        "    elif choice == '2':\n",
        "        seed = input(\"Enter seed text (3-5 words recommended): \").strip()\n",
        "        length = int(input(\"Generation length (words): \"))\n",
        "        temp = float(input(\"Temperature (0.1-1.5): \"))\n",
        "\n",
        "        print(\"\\nGenerating...\")\n",
        "        result = model_transformer.generate(word_tokenizer, seed, length, temp)\n",
        "        print(\"\\nGenerated Text:\")\n",
        "        print(result)\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid choice!\")\n",
        "\n",
        "# Uncomment to run interactive demo:\n",
        "# interactive_generation()\n",
        "\n",
        "# Or run automatic examples:\n",
        "print(\"Running automatic examples...\\n\")\n",
        "\n",
        "examples = [\n",
        "    \"the future of artificial intelligence\",\n",
        "    \"students should learn\",\n",
        "    \"data privacy is important\"\n",
        "]\n",
        "\n",
        "print(\"LSTM Examples:\")\n",
        "for ex in examples:\n",
        "    print(f\"\\nSeed: '{ex}'\")\n",
        "    print(generate_text(model_lstm, tokenizer, ex, 100, 0.8)[:150])\n",
        "\n",
        "print(\"\\n\\nTransformer Examples:\")\n",
        "for ex in examples:\n",
        "    print(f\"\\nSeed: '{ex}'\")\n",
        "    print(model_transformer.generate(word_tokenizer, ex, 15, 0.8))"
      ],
      "metadata": {
        "id": "Wr1BHyjHg6QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oMQ4M1HRgpDP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}